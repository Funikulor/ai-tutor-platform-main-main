# Быстрый старт: Подключение локальной нейросети

## Шаг 1: Установка Ollama

### Вариант А: Автоматическая установка (рекомендуется)
Просто запустите файл `install_ollama.bat` в корне проекта:
```bash
install_ollama.bat
```

### Вариант Б: Ручная установка
1. Скачайте Ollama с официального сайта: https://ollama.com/download
2. Установите Ollama (следуйте инструкциям установщика)
3. После установки Ollama автоматически запустится как служба

## Шаг 2: Установка модели

Откройте PowerShell или командную строку и выполните:

```bash
# Рекомендуемая модель (быстрая, ~2GB)
ollama pull llama3.2

# Или для лучшей поддержки русского языка
ollama pull qwen2.5:1.5b
```

Проверьте, что модель установлена:
```bash
ollama list
```

## Шаг 3: Настройка Backend

Создайте файл `.env` в папке `AdaptEd/backend/` со следующим содержимым:

```env
ASSISTANT_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

**Важно:** Если вы установили другую модель (например, `qwen2.5:1.5b`), укажите её в `OLLAMA_MODEL`:
```env
OLLAMA_MODEL=qwen2.5:1.5b
```

## Шаг 4: Запуск

1. **Убедитесь, что Ollama запущена:**
   ```bash
   ollama list
   ```
   Если команда работает, Ollama запущена.

2. **Запустите Backend:**
   ```bash
   start_backend.bat
   ```
   или вручную:
   ```bash
   cd AdaptEd/backend
   python -m uvicorn app:app --host 127.0.0.1 --port 8000 --reload
   ```

3. **Запустите Frontend:**
   ```bash
   start_frontend.bat
   ```
   или вручную:
   ```bash
   cd AdaptEd/frontend
   npm run dev
   ```

## Шаг 5: Тестирование

1. Откройте браузер: http://localhost:3000
2. Войдите в систему (или зарегистрируйтесь)
3. Перейдите в раздел "Чат с ИИ"
4. Напишите сообщение и проверьте, что нейросеть отвечает

## Проверка работы Ollama

Если хотите протестировать Ollama напрямую:

```bash
ollama run llama3.2 "Привет! Как дела?"
```

Если Ollama отвечает, значит всё работает правильно.

## Решение проблем

### Ошибка: "Ollama недоступна"
- Убедитесь, что Ollama запущена: `ollama list`
- Если не запущена, запустите: `ollama serve`
- Проверьте, что порт 11434 не занят

### Ошибка: "Модель не найдена"
- Проверьте установленные модели: `ollama list`
- Установите модель: `ollama pull llama3.2`
- Убедитесь, что название модели в `.env` совпадает с установленной

### Медленные ответы
- Используйте более легкую модель (llama3.2 вместо llama3.2:3b)
- Убедитесь, что у вас достаточно RAM (минимум 4GB для llama3.2)
- Закройте другие тяжелые приложения

### Backend не подключается к Ollama
- Проверьте файл `.env` в `AdaptEd/backend/`
- Убедитесь, что `OLLAMA_URL=http://localhost:11434`
- Перезапустите backend после изменения `.env`

## Рекомендуемые модели

| Модель | Размер | Скорость | Качество | Русский язык |
|--------|--------|----------|----------|--------------|
| llama3.2 | ~2GB | ⚡⚡⚡ | ⭐⭐⭐ | ⭐⭐ |
| qwen2.5:1.5b | ~2GB | ⚡⚡⚡ | ⭐⭐⭐ | ⭐⭐⭐ |
| qwen2.5:7b | ~4GB | ⚡⚡ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| mistral | ~4GB | ⚡⚡ | ⭐⭐⭐⭐ | ⭐⭐ |

Для начала рекомендую `llama3.2` или `qwen2.5:1.5b` - они быстрые и хорошо работают.



