# Настройка Ollama для локальной нейросети

Ollama позволяет запускать большие языковые модели локально на вашем компьютере без необходимости подключения к интернету.

## Установка Ollama

### Windows
1. Скачайте установщик с официального сайта: https://ollama.com/download
2. Запустите установщик и следуйте инструкциям
3. После установки Ollama автоматически запустится как служба

### Альтернативный способ (через PowerShell)
```powershell
# Скачайте и установите через winget (если доступен)
winget install Ollama.Ollama
```

## Запуск Ollama

После установки Ollama должен автоматически запуститься. Если нет, запустите вручную:

```powershell
ollama serve
```

Ollama будет доступна по адресу: `http://localhost:11434`

## Установка модели

Рекомендуемые модели для образовательного чата:

### Llama 3.2 (рекомендуется, ~2GB)
```bash
ollama pull llama3.2
```

### Mistral (альтернатива, ~4GB)
```bash
ollama pull mistral
```

### Qwen2.5 (хорошо работает на русском, ~2GB)
```bash
ollama pull qwen2.5:1.5b
```

### Для более мощных компьютеров:
```bash
ollama pull llama3.2:3b    # Более качественные ответы
ollama pull qwen2.5:7b     # Лучше для русского языка
```

## Настройка backend

Создайте файл `.env` в папке `AdaptEd/backend/` со следующим содержимым:

```env
ASSISTANT_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

Или используйте переменные окружения:
- `ASSISTANT_PROVIDER=ollama` - использовать Ollama
- `OLLAMA_URL=http://localhost:11434` - URL Ollama сервера
- `OLLAMA_MODEL=llama3.2` - название модели

## Проверка работы

1. Убедитесь, что Ollama запущена:
```bash
ollama list
```

2. Протестируйте модель:
```bash
ollama run llama3.2 "Привет! Как дела?"
```

3. Запустите backend и проверьте чат в интерфейсе

## Решение проблем

### Ollama не запускается
- Проверьте, что порт 11434 не занят другим приложением
- Перезапустите службу Ollama

### Модель не отвечает
- Убедитесь, что модель установлена: `ollama list`
- Проверьте логи backend на наличие ошибок
- Попробуйте другую модель

### Медленные ответы
- Используйте более легкую модель (например, `llama3.2` вместо `llama3.2:3b`)
- Убедитесь, что у вас достаточно RAM (минимум 4GB для llama3.2)
- Закройте другие тяжелые приложения

## Дополнительные модели

Для русского языка также хорошо работают:
- `qwen2.5:1.5b` - быстрая, хорошо понимает русский
- `qwen2.5:7b` - более качественные ответы
- `saiga3` - специально обученная на русском языке

Установка:
```bash
ollama pull qwen2.5:1.5b
ollama pull saiga3
```



